---
layout: post
title: Two papers accepted at UAI 2022
date: 2022-06-26 12:00:00 UTC
disqus_comments: false
photo_url: "/public/amsterdam.jpeg"
---

Two of our papers are accepted at the Conference on Uncertainty in Artificial Intelligence (UAI 2022).

- [*StackMix: A complementary Mix algorithm*](https://openreview.net/pdf?id=HqIlPIUo5g9)

>**Abstract.** 
>Techniques combining multiple images as input/output have proven to be effective data augmentations for training convolutional neural networks. In this paper, we present StackMix: each input is presented as a concatenation of two images, and the label is the mean of the two one-hot labels. On its own, StackMix rivals other widely used methods in the ``Mix'' line of work. More importantly, unlike previous work, significant gains across a variety of benchmarks are achieved by combining StackMix with existing Mix augmentation, effectively mixing more than two images. E.g., by combining StackMix with CutMix, test error in the supervised setting is improved across a variety of settings over CutMix, including 0.8\% on ImageNet, 3\% on Tiny ImageNet, 2\% on CIFAR-100, 0.5\% on CIFAR-10, and 1.5\% on STL-10. Similar results are achieved with Mixup. We further show that gains hold for robustness to common input corruptions and perturbations at varying severities with a 0.7\% improvement on CIFAR-100-C, by combining StackMix with AugMix over AugMix. On its own, improvements with StackMix hold across different number of labeled samples on CIFAR-100, maintaining approximately a 2\% gap in test accuracy --down to using only 5\% of the whole dataset-- and is effective in the semi-supervised setting with a 2\% improvement with the standard benchmark -model. Finally, we perform an extensive ablation study to better understand the proposed methodology.


- [*ResIST: Layer-Wise Decomposition of ResNets for Distributed Training*](https://openreview.net/pdf?id=rq4v8Ujcec)

>**Abstract.** 
>We propose ResIST, a novel distributed training protocol for Residual Networks (ResNets). ResIST randomly decomposes a global ResNet into several shallow sub-ResNets that are trained independently in a distributed manner for several local iterations, before having their updates synchronized and aggregated into the global model. In the next round, new sub-ResNets are randomly generated and the process repeats until convergence. By construction, per iteration, ResIST communicates only a small portion of network parameters to each machine and never uses the full model during training. Thus, ResIST reduces the per-iteration communication, memory, and time requirements of ResNet training to only a fraction of the requirements of full-model training. In comparison to common protocols, like data-parallel training and data-parallel training with local SGD, ResIST yields a decrease in communication and compute requirements, while being competitive with respect to model performance.
