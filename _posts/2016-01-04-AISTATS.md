---
layout: post
disqus_comments: false
date: 2016-01-04 20:00:00 UTC
title: AISTATS 2016
photo_url: "http://www.aistats.org/image/Old-City-Cadiz.jpg"
---

Three papers are accepted at AI & Statistics (AISTATS) conference, that will be held
in Cadiz, Spain. 

- *Learning sparse additive models with interactions in high dimensions*: 
Joint work with [*Hemant Tyagi*](http://people.inf.ethz.ch/htyagi/), [*Bernd Gartner*](http://people.inf.ethz.ch/gaertner/) 
and [*Andreas Krause*](https://las.inf.ethz.ch/krausea).

<pre>**Abstract.** 
A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is referred to as a Sparse Additive Model (SPAM), if it is of the
form $f(\mathbf{x}) = \sum\_{l \in \mathcal{S}}\phi\_{l}(x\_l)$, where $\mathcal{S} \subset [d]$, $|\mathcal{S}| \ll d$.
Assuming $\phi\_l$'s and $\mathcal{S}$ to be unknown, the problem of estimating $f$ from its samples has been
studied extensively. In this work, we consider a generalized SPAM, allowing for *second order* interaction terms.
For some $\mathcal{S}\_1 \subset [d], \mathcal{S}\_2 \subset {[d] \choose 2}$, the function $f$ is assumed to be of the form: 
$f(\mathbf{x}) = \sum\_{p \in \mathcal{S}\_1}\phi\_{p} (x\_p) + \sum\_{(l\_1, l\_2) \in \mathcal{S}\_2}\phi\_{(l\_1, l\_2)} \mathbf{x}\_{(l\_1, l\_2)}.$
Assuming $\phi\_{p},\phi\_{(l\_1, l\_2)}$, $\mathcal{S}\_1$ and, $\mathcal{S}\_2$ to be unknown,
we provide a randomized algorithm that queries $f$ and  
*exactly recovers* $\mathcal{S}\_1,\mathcal{S}\_2$. Consequently, this also enables us to estimate the underlying 
$\phi\_p, \phi\_{(l\_1, l\_2)}$. We derive sample complexity bounds for our scheme and also extend our analysis
to include the situation where the queries are corrupted with noise -- either stochastic, 
or arbitrary but bounded. Lastly, we provide simulation results on synthetic data, that
validate our theoretical findings.</pre>

- *Convex block-sparse linear regression with expanders, provably*:
Joint work with [*Bubacarr Bah*](https://www.ma.utexas.edu/users/bah/), Rouzbeh Hasheminezhad, [*Quoc Tran-Dinh*](http://trandinhquoc.com/), 
[*Luca Baldassarre*](http://people.epfl.ch/226590), and [*Volkan Cevher*](http://lions.epfl.ch/cms/site/lions2/lang/en/volkan.cevher).

<pre>**Abstract.** 
Sparse matrices are favorable objects in machine learning and optimization. When such matrices are used, 
in spite of dense ones, the overall complexity requirements in optimization can be significantly reduced 
in practice, both in terms of space and run-time. 
Prompted by this observation, we study a convex optimization scheme for block-sparse recovery from 
linear measurements. 
To obtain linear sketches, we use expander matrices, *i.e.*, sparse matrices containing only 
few non-zeros per column. Hitherto, to the best of our knowledge, such algorithmic solutions have 
been only studied from a non-convex perspective. Our aim here is to theoretically characterize the 
performance of convex approaches under such setting. 

Our key novelty is the expression of the recovery error in terms of the model-based norm, while 
assuring that solution leaves in the model. 
To achieve this, we show that sparse model-based matrices satisfy a \emph{group} version of the null-space property. 
Our experimental findings on synthetic and real applications support our claims for faster 
recovery in the convex setting -- as opposed to using dense sensing matrices, while showing a competitive recovery performance. </pre>
