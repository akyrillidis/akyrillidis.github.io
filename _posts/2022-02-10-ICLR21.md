---
layout: post
title: PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication is accepted at ICLR 2022
date: 2022-02-10 12:00:00 UTC
disqus_comments: false
---

Our paper on PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication is accepted at the Tenth International Conference on Learning Representations (ICLR 2022).

- [*PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication*](/pubs/Conferences/PipeGCN.pdf): 

>**Abstract.** 
>Graph Convolutional Networks (GCNs) is the state-of-the-art method for learning graph-structured data, and training large-scale GCNs requires distributed training across multiple accelerators such that each accelerator is able to hold a partitioned subgraph. However, distributed GCN training incurs prohibitive overhead of communicating node features and feature gradients among partitions for every GCN layer in each training iteration, limiting the achievable training efficiency and model scalability. To this end, we propose PipeGCN, a simple-yet-effective scheme that hides the communication overhead by pipelining inter-partition communication with intra-partition computation. It is non-trivial to pipeline for efficient GCN training, as communicated node features/gradients will become stale and thus can harm the convergence, negating the pipeline benefit. Notably, little is known regarding the convergence rate of GCN training with both stale features and stale feature gradients. This work not only provides a theoretical convergence guarantee but also finds the convergence rate of PipeGCN to be close to that of the vanilla distributed GCN training without staleness. Furthermore, we develop a smoothing method to further improve PipeGCN's convergence. Extensive experiments show that PipeGCN can largely boost training throughput (up to 2.2 x) while achieving the same accuracy as its vanilla counterpart and outperforming existing full-graph training methods. All code will be released publicly upon acceptance.